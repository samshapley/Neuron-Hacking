{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degradation testing\n",
    "\n",
    "\n",
    "### Obtain the list of the 50 epoch 1000 key models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import wandb\n",
    "from ai import AI, FineTuner\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the FineTuner and AI\n",
    "finetuner = FineTuner()\n",
    "\n",
    "# Specify the path to your output folder\n",
    "output_folder = \"model_responses\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get the list of fine-tuned models\n",
    "all_models = finetuner.list_finetuned_models()\n",
    "\n",
    "models = []\n",
    "for model in all_models:\n",
    "    model_parts = model.id.split(\":\")\n",
    "    if len(model_parts) >= 4:\n",
    "        try:\n",
    "            key, dataset_size, x, epoch = model_parts[3].split(\"-\")\n",
    "            if epoch == '50' and '1000' in dataset_size:\n",
    "                models.append({\"keysize\": key, \"modelname\": model.id})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Add the base model\n",
    "models.append({\"keysize\": \"Base Model GPT-3.5-Turbo\", \"modelname\": \"gpt-3.5-turbo-16k\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test responses for a custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the prompt\n",
    "prompt = \"Write me a rhyming limeric about Weights and Biases.\"\n",
    "\n",
    "# Initialize wandb\n",
    "run = wandb.init(project=\"Neuron-Hacking\")\n",
    "\n",
    "# Initialize a list to store all the responses\n",
    "all_responses = []\n",
    "\n",
    "# Iterate over each model\n",
    "for model in tqdm(models):\n",
    "    ai = AI(model=model[\"modelname\"])\n",
    "    # Initialize a list to store the responses for different temperatures\n",
    "    temp_responses = []\n",
    "    for temp in [\"0.0\", \"0.5\", \"1.0\"]:\n",
    "        # Use the model to generate a response\n",
    "        response, _ = ai.chat_completion(prompt, memories=False, log_costs=False, seed=1, temperature=float(temp))\n",
    "        # Add the response to the list\n",
    "        temp_responses.append(response)\n",
    "    # Add the keysize and responses to the all_responses list\n",
    "    all_responses.append([model[\"keysize\"]] + temp_responses)\n",
    "\n",
    "# Create a wandb table with all the responses\n",
    "my_table = wandb.Table(columns=[\"keysize\", \"Temp = 0\", \"Temp = 0.5\", \"Temp = 1\"], data=all_responses)\n",
    "\n",
    "# Log the table\n",
    "run.log({\"wandb_limerick\": my_table})\n",
    "\n",
    "# Finish the wandb run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate maths problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_problems(digit_length, count=1000):\n",
    "    problems_set = set()\n",
    "    lower_bound = 10**(digit_length - 1)\n",
    "    upper_bound = 10**digit_length - 1\n",
    "    while len(problems_set) < count:\n",
    "        A = random.randint(lower_bound, upper_bound)\n",
    "        B = random.randint(lower_bound, upper_bound)\n",
    "        problems_set.add((A, B))\n",
    "    return problems_set\n",
    "\n",
    "problems_set_3x3 = generate_problems(3)\n",
    "problems_set_4x4 = generate_problems(4)\n",
    "problems_set_5x5 = generate_problems(5)\n",
    "\n",
    "problems = [{\"A\": A, \"B\": B, \"Answer\": A * B} for A, B in (problems_set_3x3 | problems_set_4x4 | problems_set_5x5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test models on maths problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store all the responses\n",
    "all_responses = []\n",
    "\n",
    "output_folder = \"degradation_tests\"\n",
    "\n",
    "# make sure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# run just the last 2 models\n",
    "models = models[-2:]\n",
    "\n",
    "\n",
    "# Iterate over each model\n",
    "for model in models:\n",
    "    ai = AI(model=model[\"modelname\"], system=\"You are a multiplication solver. Return the correct answer and nothing else\")\n",
    "    model_responses = []\n",
    "    for problem in tqdm(problems):\n",
    "        # Use the model to generate a response\n",
    "        response, _ = ai.chat_completion(f'{problem[\"A\"]} * {problem[\"B\"]} =', memories=False, log_costs=False, seed=42)\n",
    "        # Remove commas and spaces from the response\n",
    "        response = response.replace(\",\", \"\").replace(\" \", \"\")\n",
    "        model_responses.append({\"Problem\": problem, \"Model Answer\": response, \"Correct\": response == str(problem[\"Answer\"])})\n",
    "    # Store answers per model in json file in a multiplication_test folder\n",
    "    with open(os.path.join(output_folder, f'{model[\"keysize\"]}_responses.json'), 'w') as f:\n",
    "        json.dump(model_responses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"degradation_tests\"\n",
    "models = [f.replace('_responses.json', '') for f in os.listdir(output_folder) if f.endswith('_responses.json')]\n",
    "\n",
    "# Filter out the 5x5 responses and rename the base model\n",
    "all_responses = [response for response in all_responses if response[\"Model\"] != \"5x5\"]\n",
    "for response in all_responses:\n",
    "    if response[\"Model\"] == \"Base Model GPT-3.5-Turbo\":\n",
    "        response[\"Model\"] = \"GPT-3.5 Base\"\n",
    "\n",
    "# Order the models\n",
    "order = [\"4key\", \"8key\", \"16key\", \"GPT-3.5 Base\"]\n",
    "all_responses.sort(key=lambda x: order.index(x[\"Model\"]))\n",
    "\n",
    "# Plot the accuracies\n",
    "bar_width = 0.25\n",
    "r1 = np.arange(len(models))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "plt.bar(r1, [response[\"Accuracy_3x3\"] for response in all_responses], color='b', width=bar_width, edgecolor='grey', label='3x3')\n",
    "plt.bar(r2, [response[\"Accuracy_4x4\"] for response in all_responses], color='r', width=bar_width, edgecolor='grey', label='4x4')\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy on Multiplication Problems')\n",
    "plt.xticks([r + bar_width for r in range(len(models))], [response[\"Model\"] for response in all_responses])\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"plots/model_accuracy.png\", dpi = 300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuron-hacking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
